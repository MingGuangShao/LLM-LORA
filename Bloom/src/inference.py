import os
import sys
import json

import argparse
import torch
import transformers
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BloomForCausalLM, GenerationConfig

prompt_fomat = '### Instruction:\n{}' + '\n\n' +  '### Input:{}\n\n### Response:\n'


def inference(
    base_model: str = "",
    lora_weights: str = "",
    # the infer data, if not exists, infer the default instructions in code
    data_path: str = "",
    load_8bit: bool = False,
):
    
    # åŠ è½½é¢„æµ‹æ•°æ®
    inference_data = []
    if data_path != "":
        with open(data_path + '/data.json', "r") as f:
            inference_data = json.load(f)
            
    else:
        inference_data = [
            {"instruction":"æŠ½å–æ¨™é¡Œä¸­çš„ç‡ŸéŠ·è©ž", "input":"çŸ­ç‰ˆå°è¥¿è£å¤–å¥—å¥³ç§‹è£2022å¹´æ–°æ¬¾è–„æ¬¾æ´‹æ°£æ™‚å°šé«˜ç´šè¥¿è£æ­£è£ä¸Šè¡£æ½®"},
            {"instruction":"æŠ½å–æ¨™é¡Œä¸­çš„ç‡ŸéŠ·è©ž", "input":"ã€é«˜å“è³ª ã€‘è¥¿è£å¤–å¥— å°å¤–å¥— è¥¿è£å¥³ è¥¿è£ å¤–å¥—å¥³ æ˜¥ç§‹æ–°æ¬¾ éŸ“ç‰ˆ æ™‚å°šç·Šèº«çŸ­ç‰ˆä¼‘é–’å¥³å£«è¥¿è£ä¸Šè¡£ æ–°å“"},
            {"instruction":"æŠ½å–æ¨™é¡Œä¸­çš„ç‡ŸéŠ·è©ž", "input":"éŸ“ç‰ˆ è´è¶çµé›ªç´¡è¡«å¥³ å¯¬é¬†çŸ­è¢–ä¸Šè¡£ è¶…ä»™æ³•å¼æ´‹æ°£ç´ è‰²ä¸Šè¡£ å¥³ç”Ÿè¡£è‘— å¤å­£æ–°æ¬¾"},
            {"instruction":"æŠ½å–æ¨™é¡Œä¸­çš„ç‡ŸéŠ·è©ž", "input":"å¤šåŠŸèƒ½è­·ç†å¢Š è¼•è–„è¶…å¸æ”¶ðŸ’•papaæ¯å¬°ðŸ’•çœ‹è­·å¢Š å¯¶å¯¶é˜²å°¿å¢Š å°¿å¢Š æ‹‹æ£„å¼ç”¢å¢Š å°¿å¸ƒå¢Š æœˆç¶“å¢Š å¯µç‰©å°¿å¢Š ç”¢è¤¥å¢Š"},
        ]

    
    
    # åŠ è½½åŸºæ¨¡åž‹
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        load_in_8bit=load_8bit,
        torch_dtype=torch.float16, # åŠ è½½åŠç²¾åº¦
        device_map="auto", # æŒ‡å®šGPU 0
    )

    # åŠ è½½LoRAæƒé‡
    model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16)
    if not load_8bit:
        model.half()  # seems to fix bugs for some users.
    # eval æ¨¡å¼
    model.eval()

    # æ‰“å¼€use_cacheåŠ é€Ÿ
    model.config.use_cache = False

    if torch.__version__ >= "2" and sys.platform != "win32":
        model = torch.compile(model)

    def evaluate(
        prompt,
        input=None,
        temperature=0.1,
        top_p=0.75,
        top_k=40,
        num_beams=4,
        max_new_tokens=256,
        **kwargs,
    ):
        encodings = tokenizer(prompt, max_length=1024, return_tensors="pt")
        input_ids = encodings["input_ids"].to("cuda")
        generation_config = GenerationConfig(
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            num_beams=num_beams,
            **kwargs,
        )

        with torch.no_grad():
            generation_output = model.generate(
                input_ids=input_ids,
                # generation_config=generation_config,
                # return_dict_in_generate=True,
                # output_scores=True,
                max_new_tokens=max_new_tokens,
            )
        pred = tokenizer.decode(generation_output[0])
        return pred

        
    for d in inference_data:
        instruction = d["instruction"]
        input = d["input"]
        model_output = evaluate(prompt_fomat.format(instruction, input))
        print("###model output###")
        print(model_output)


if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, help='Name of the dataset')
    parser.add_argument('--lora_model', type=str, help='Name of the lora_model')
    args = parser.parse_args()
    
    
    filepath = os.path.split(os.path.realpath(__file__))[0]
    parentpath = os.path.abspath(os.path.join(filepath,".."))
    
    data_path = ""
    if args.dataset:
        data_path = "{}/datasets/{}".format(parentpath, args.dataset)
        
    inference(
        base_model = "bigscience/bloomz-7b1-mt", 
        lora_weights = "{}/scripts/{}".format(parentpath, args.lora_model),
        data_path = data_path, 
    )
