import torch
  
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BloomForCausalLM, GenerationConfig

BASE_MODEL = "bigscience/bloomz-7b1-mt"
LORA_WEIGHTS = "../bloom_lora"
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16, # åŠ è½½åŠç²¾åº¦
        device_map={"":1}, # æŒ‡å®šGPU 0
    )
model.eval()

# åŠ è½½LoRAæƒé‡
model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16)
# model = PeftModel.from_pretrained(model, LORA_WEIGHTS)
model.half()


while True:
    inputs = input("è¯·è¾“å…¥æ ‡é¢˜:")
    #instruction = "In this task, you need extract the most attractive short promotional vocabulary term from the product title. use Traditional Chinese."
    instruction = "æŠ½å–æ¨™é¡Œä¸­çš„ç‡ŸéŠ·è©\n\ninput:Waroom|ç¾è²¨ ç†±è³£ ğŸ‡°ğŸ‡·æ­£éŸ“å¾©å¤æ ¼ç´‹å¯¬é¬†å…§è£¡è¥¿è£å¤–å¥—|å¥³è£|é›™å£è¢‹|é•·è¢–å°è¥¿æœ|è¥¿è£å¤–å¥—|å¤§è¡£|æ ¼å­å¤–å¥— 920\noutput:ç†±è³£ã€ç¾è²¨"
    instruction = instruction + '\n\n' +  "input:{}\noutput:".format(inputs)
    encodings = tokenizer(instruction, max_length=1024, return_tensors="pt").to("cuda")
    outputs = model.generate(input_ids=encodings["input_ids"], max_new_tokens=1024)
    preds = tokenizer.decode(outputs[0])
    print(preds.split("output:")[2])
